{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1946405f",
   "metadata": {},
   "source": [
    "# Credit Risk Modeling — Notebook\n",
    "\n",
    "**Goal:** Assess creditworthiness of loan applicants (binary classification: good / bad loan).\n",
    "\n",
    "This notebook includes: environment setup, EDA examples, preprocessing, SMOTE, model training (Logistic Regression / Random Forest / XGBoost), evaluation (ROC AUC, PR AUC, confusion matrix), and SHAP explainability. Update `DATA_PATH` to point to your CSV file and run cells in order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735cf07",
   "metadata": {},
   "source": [
    "## 1) Install requirements (run once)\n",
    "\n",
    "Run the following in your environment if packages are missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install pandas numpy scikit-learn imbalanced-learn xgboost shap matplotlib seaborn joblib nbformat\n",
    "print('Skip pip install in notebook if already installed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf23167",
   "metadata": {},
   "source": [
    "## 2) Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c063e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, classification_report, confusion_matrix\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "print('Libraries imported.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194f081",
   "metadata": {},
   "source": [
    "## 3) Configuration: dataset path and quick target mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352280f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === EDIT THIS: point to your dataset CSV ===\n",
    "DATA_PATH = 'lending_club_loans.csv'  # change to your CSV file path\n",
    "\n",
    "# Example load (no internet - ensure file exists)\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "    print('Loaded:', DATA_PATH, 'shape=', df.shape)\n",
    "except FileNotFoundError:\n",
    "    print('File not found at', DATA_PATH)\n",
    "    df = pd.DataFrame()  # placeholder\n",
    "\n",
    "# Quick target creation helper\n",
    "if not df.empty:\n",
    "    if 'target' not in df.columns:\n",
    "        if 'loan_status' in df.columns:\n",
    "            df['target'] = df['loan_status'].apply(lambda x: 1 if x in ['Charged Off','Default'] else 0)\n",
    "            print('Derived target from loan_status. Value counts:\\n', df['target'].value_counts(dropna=False))\n",
    "        else:\n",
    "            print('No `target` or `loan_status` column found. Please create a binary target column named `target`.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630e6c6",
   "metadata": {},
   "source": [
    "## 4) Quick EDA examples\n",
    "\n",
    "These are example exploratory analyses — run them when your dataset is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ef3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not df.empty:\n",
    "    display(df.head())\n",
    "    print('\\nNumeric summary:')\n",
    "    display(df.describe().T)\n",
    "    print('\\nTarget distribution:')\n",
    "    display(df['target'].value_counts(normalize=True))\n",
    "    \n",
    "    # Example: top features missingness\n",
    "    missing = df.isnull().mean().sort_values(ascending=False).head(20)\n",
    "    print('\\nTop 20 columns by missing rate:')\n",
    "    display(missing)\n",
    "    \n",
    "    # Example: plot target vs a numeric feature if exists\n",
    "    numeric_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "    if numeric_cols:\n",
    "        col = numeric_cols[0]\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.histplot(data=df, x=col, hue='target', bins=50, stat='density', element='step', common_norm=False)\n",
    "        plt.title(f'Distribution of {col} by target')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('Load a dataset to run EDA.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e1521",
   "metadata": {},
   "source": [
    "## 5) Preprocessing, SMOTE, and model training (example)\n",
    "\n",
    "This cell constructs a preprocessing pipeline, applies SMOTE during training, and runs GridSearch on RandomForest. Modify features list as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be84770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if df.empty:\n",
    "    print('Dataset not loaded. Please update DATA_PATH and run the earlier cell.')\n",
    "else:\n",
    "    # Simple feature selection: drop ids and text heavy columns\n",
    "    drop_cols = [c for c in ['id','member_id','url','desc','title'] if c in df.columns]\n",
    "    X = df.drop(columns=drop_cols + ['target'], errors='ignore')\n",
    "    y = df['target']\n",
    "    \n",
    "    # Identify feature types\n",
    "    num_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "    cat_features = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "    \n",
    "    # Build preprocessors (adapt strategies as needed)\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, num_features),\n",
    "        ('cat', categorical_transformer, cat_features)\n",
    "    ], remainder='drop')\n",
    "    \n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    rf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    \n",
    "    pipe = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('smote', smote),\n",
    "        ('clf', rf)\n",
    "    ])\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE)\n",
    "    print('Train/test shapes:', X_train.shape, X_test.shape)\n",
    "    \n",
    "    # Grid search (small example grid)\n",
    "    param_grid = {\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__max_depth': [6, 12]\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "    search = GridSearchCV(pipe, param_grid, scoring='roc_auc', n_jobs=-1, cv=cv, verbose=1)\n",
    "    search.fit(X_train, y_train)\n",
    "    print('Best params:', search.best_params_)\n",
    "    \n",
    "    # Evaluation\n",
    "    best_model = search.best_estimator_\n",
    "    probs = best_model.predict_proba(X_test)[:,1]\n",
    "    preds = best_model.predict(X_test)\n",
    "    roc = roc_auc_score(y_test, probs)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, probs)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    print(f'Test ROC AUC: {roc:.4f} | PR AUC: {pr_auc:.4f}')\n",
    "    print('\\nClassification report:')\n",
    "    print(classification_report(y_test, preds))\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(best_model, 'credit_model_rf.joblib')\n",
    "    print('Saved model to credit_model_rf.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878dafa",
   "metadata": {},
   "source": [
    "## 6) SHAP explainability (summary + single prediction)\n",
    "\n",
    "Note: for large feature spaces KernelExplainer can be slow. For tree models TreeExplainer is used here on the preprocessed numeric matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if df.empty:\n",
    "    print('Dataset not loaded.')\n",
    "else:\n",
    "    # Refit a preprocessor-only to get transformed arrays and feature names\n",
    "    preproc_only = preprocessor.fit(X_train)\n",
    "    X_train_trans = preproc_only.transform(X_train)\n",
    "    X_test_trans = preproc_only.transform(X_test)\n",
    "    \n",
    "    # Helper to extract feature names from ColumnTransformer\n",
    "    def get_feature_names_from_ct(ct):\n",
    "        feature_names = []\n",
    "        for name, trans, cols in ct.transformers_:\n",
    "            if name == 'remainder': \n",
    "                continue\n",
    "            if hasattr(trans, 'named_steps') and 'onehot' in trans.named_steps:\n",
    "                ohe = trans.named_steps['onehot']\n",
    "                cats = ohe.categories_\n",
    "                for i, col in enumerate(cols):\n",
    "                    for cat in cats[i]:\n",
    "                        feature_names.append(f\"{col}__{cat}\")\n",
    "            else:\n",
    "                feature_names.extend(cols)\n",
    "        return feature_names\n",
    "    \n",
    "    feature_names = get_feature_names_from_ct(preproc_only)\n",
    "    clf = best_model.named_steps['clf']\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(clf)\n",
    "        shap_vals = explainer.shap_values(X_test_trans)\n",
    "        # shap_vals may be list for classifiers\n",
    "        if isinstance(shap_vals, list):\n",
    "            shap_vals_pos = shap_vals[1]\n",
    "        else:\n",
    "            shap_vals_pos = shap_vals\n",
    "        print('Displaying SHAP summary plot (this will render inline in Jupyter):')\n",
    "        shap.summary_plot(shap_vals_pos, features=X_test_trans, feature_names=feature_names, show=True)\n",
    "    except Exception as e:\n",
    "        print('SHAP explanation failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dff5642",
   "metadata": {},
   "source": [
    "## 7) Next steps\n",
    "\n",
    "- Tune preprocessing and feature engineering (binning, debt-to-income ratio, credit history features).\n",
    "- Try XGBoost/LightGBM and compare with calibration.\n",
    "- Add fairness checks for protected attributes.\n",
    "- Export model + preprocessors for production use (pickle / joblib)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
